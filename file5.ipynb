{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import *\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping, ReduceLROnPlateau\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.metrics import top_k_categorical_accuracy\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import hamming_loss, f1_score\n",
    "from keras.optimizers import SGD,RMSprop,Adam\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import label_ranking_average_precision_score, coverage_error, label_ranking_loss\n",
    "import pickle\n",
    "import json\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "from keras.losses import binary_crossentropy,mean_squared_error\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "np.random.seed(31)\n",
    "tf.set_random_seed(17)\n",
    "\n",
    "CLEAN_DF_BASE = ''\n",
    "EMB_BASE = ''\n",
    "\n",
    "df = pd.read_csv( CLEAN_DF_BASE + 'cleaned_train_punc_brute.csv')\n",
    "df_test = pd.read_csv( CLEAN_DF_BASE + 'cleaned_test_punc_brute.csv')\n",
    "\n",
    "MAX_NB_WORDS = 100000\n",
    "MAX_SEQUENCE_LENGTH = 400\n",
    "VALIDATION_SAMPLES = 5000\n",
    "EMBEDDING_DIM = 300\n",
    "BATCH_SIZE = 512\n",
    "MAX_TAGS = 512\n",
    "\n",
    "with open(EMB_BASE + 'word_index_punc_brute','r') as f:\n",
    "    word_index = json.load(f)\n",
    "print(word_index.get(\"python\"))\n",
    "\n",
    "def read_label(raw_data):\n",
    "    \n",
    "    l = len(raw_data)\n",
    "    label_list = [] #np.empty((raw_data.shape[0],len(tagCol)))\n",
    "    ll =[]\n",
    "    tagCol = [col for col in raw_data.columns if 'Tag_' in col]\n",
    "    for j in range(0,raw_data.shape[0]):\n",
    "        for i in range(0,len(tagCol)):\n",
    "            if(raw_data.loc[raw_data.index[j],tagCol[i]]==1):\n",
    "                ll.append(tagCol[i])\n",
    "                #print(ll)\n",
    "        label_list.append(ll)\n",
    "        ll = []\n",
    "        #print(label_list)\n",
    "    return label_list\n",
    "\n",
    "label_list = read_label(df)\n",
    "label_list\n",
    "\n",
    "df['tagslist'] = label_list\n",
    "    \n",
    "#df['tagslist'] = df.apply(lambda row: read_label(row), axis=1)\n",
    "label_counts = {}\n",
    " \n",
    "for tags in df['tagslist']:\n",
    "    for tag in tags:\n",
    "        try:\n",
    "            label_counts[tag] += 1\n",
    "        except:\n",
    "            label_counts[tag] = 1\n",
    "all_sorted = sorted(label_counts.items(),key=lambda x : x[1],reverse=True)\n",
    "topk = all_sorted[:MAX_TAGS]\n",
    "topk = dict(topk)\n",
    "total_all = 0\n",
    "for key,val in all_sorted:\n",
    "    total_all += val\n",
    "total_k = 0\n",
    "for key,val in topk.items():\n",
    "    total_k += val\n",
    "total_k/total_all\n",
    "\n",
    "def topkTags(row):\n",
    "    tags = []\n",
    "    for tag in row.tagslist:\n",
    "        if tag in topk.keys():\n",
    "            tags.append(tag)\n",
    "    return list(set(tags))            \n",
    "    \n",
    "df['new_tagslist'] = df.apply(lambda row: topkTags(row), axis=1)\n",
    "\n",
    "label_index = {}\n",
    "i = 0\n",
    "for tags in df['new_tagslist'].values:\n",
    "    for tag in tags:\n",
    "        try:\n",
    "            label_index[tag]\n",
    "        except:\n",
    "            label_index[tag] = i\n",
    "            i += 1\n",
    "label_index.get('python')\n",
    "\n",
    "with open('label_index1','w') as f:\n",
    "    json.dump(label_index,f)\n",
    "\n",
    "def datagen(df,batch_size=128,sequence_length=100):\n",
    "    x,y = [],[]\n",
    "    while True:\n",
    "        i = 0\n",
    "        while i < len(df):\n",
    "            if len(x) < batch_size:\n",
    "                seq = []\n",
    "                for word in df['texts'][i].split():\n",
    "                    if word_index.get(word.lower()):\n",
    "                        seq.append(word_index.get(word.lower()))\n",
    "                for _ in range(sequence_length):\n",
    "                    seq.append(0)\n",
    "                if len(seq) > sequence_length:\n",
    "                    seq = seq[:sequence_length]\n",
    "                label = []\n",
    "                for tag in df['new_tagslist'][i]:\n",
    "                    label.append(label_index.get(tag.lower()))\n",
    "                label_vec = np.zeros(len(label_index))\n",
    "                for l in label:\n",
    "                    label_vec[l] += 1\n",
    "                x.append(seq)\n",
    "                y.append(label_vec)\n",
    "            else:\n",
    "                yield np.array(x),np.array(y)\n",
    "                x,y = [],[]\n",
    "            i += 1\n",
    "\n",
    "train_df,val_df = train_test_split(df,test_size=VALIDATION_SAMPLES,random_state=31)\n",
    "train_gen = datagen(train_df.reset_index(drop=True),batch_size=BATCH_SIZE,sequence_length=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "valid_gen = datagen(val_df.reset_index(drop=True),batch_size=len(val_df),sequence_length=MAX_SEQUENCE_LENGTH)\n",
    "valid_x, valid_y = [], []\n",
    "for x,y in valid_gen:\n",
    "    valid_x.extend(list(x))\n",
    "    valid_y.extend(list(y))\n",
    "    break\n",
    "valid_x = np.array(valid_x)\n",
    "valid_y = np.array(valid_y)\n",
    "valid_x.shape,valid_y.shape\n",
    "\n",
    "test_x = []\n",
    "for i in tqdm(range(len(df_test))):\n",
    "    seq = []\n",
    "    for word in df_test['texts'][i].split():\n",
    "        if word_index.get(word.lower()):\n",
    "            seq.append(word_index.get(word.lower()))\n",
    "    for _ in range(MAX_SEQUENCE_LENGTH):\n",
    "        seq.append(0)\n",
    "    if len(seq) > MAX_SEQUENCE_LENGTH:\n",
    "        seq = seq[:MAX_SEQUENCE_LENGTH]\n",
    "    test_x.append(seq)\n",
    "\n",
    "from keras.engine.topology import Layer\n",
    "#https://www.kaggle.com/qqgeogor/keras-lstm-attention-glove840b-lb-0-043\n",
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
    "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0],  self.features_dim\n",
    "\n",
    "#https://github.com/arthurdouillard/keras-snapshot_ensembles/blob/master/snapshot.py\n",
    "import math\n",
    "import os\n",
    "import glob\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "class Snapshot(Callback):\n",
    "\n",
    "    def __init__(self, folder_path, nb_epochs, nb_cycles=5, verbose=0):\n",
    "        if nb_cycles > nb_epochs:\n",
    "            raise ValueError('nb_epochs has to be lower than nb_cycles.')\n",
    "\n",
    "        super(Snapshot, self).__init__()\n",
    "        self.verbose = verbose\n",
    "        self.folder_path = folder_path\n",
    "        self.nb_epochs = nb_epochs\n",
    "        self.nb_cycles = nb_cycles\n",
    "        self.period = self.nb_epochs // self.nb_cycles\n",
    "        self.nb_digits = len(str(self.nb_cycles))\n",
    "        self.path_format = os.path.join(self.folder_path, 'weights_cycle_{}.h5')\n",
    "\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch == 0 or (epoch + 1) % self.period != 0: return\n",
    "        # Only save at the end of a cycle, a not at the beginning\n",
    "\n",
    "        if not os.path.exists(self.folder_path):\n",
    "            os.makedirs(self.folder_path)\n",
    "\n",
    "        cycle = int(epoch / self.period)\n",
    "        cycle_str = str(cycle).rjust(self.nb_digits, '0')\n",
    "        self.model.save_weights(self.path_format.format(cycle_str), overwrite=True)\n",
    "\n",
    "        # Resetting the learning rate\n",
    "        K.set_value(self.model.optimizer.lr, self.base_lr)\n",
    "\n",
    "        if self.verbose > 0:\n",
    "            print('\\nEpoch %05d: Reached %d-th cycle, saving model.' % (epoch, cycle))\n",
    "\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        if epoch <= 0: return\n",
    "\n",
    "        lr = self.schedule(epoch)\n",
    "        K.set_value(self.model.optimizer.lr, lr)\n",
    "\n",
    "        if self.verbose > 0:\n",
    "            print('\\nEpoch %05d: Snapchot modifying learning '\n",
    "                  'rate to %s.' % (epoch + 1, lr))\n",
    "\n",
    "\n",
    "    def set_model(self, model):\n",
    "        self.model = model\n",
    "        if not hasattr(self.model.optimizer, 'lr'):\n",
    "            raise ValueError('Optimizer must have a \"lr\" attribute.')\n",
    "\n",
    "        # Get initial learning rate\n",
    "        self.base_lr = float(K.get_value(self.model.optimizer.lr))\n",
    "\n",
    "\n",
    "    def schedule(self, epoch):\n",
    "        lr = math.pi * (epoch % self.period) / self.period\n",
    "        lr = self.base_lr / 2 * (math.cos(lr) + 1)\n",
    "        return lr\n",
    "\n",
    "def get_model(emb_w = None,input_layer = None):\n",
    "    inp = input_layer if input_layer is not None else Input(shape=(MAX_SEQUENCE_LENGTH,)) \n",
    "    if emb_w is None:\n",
    "        emb = Embedding(MAX_NB_WORDS,EMBEDDING_DIM)(inp)\n",
    "    else:\n",
    "        emb = Embedding(MAX_NB_WORDS,emb_w.shape[1],weights = [emb_w])(inp)\n",
    "    emb = SpatialDropout1D(0.2)(emb)\n",
    "    x = CuDNNGRU(len(label_index)//2, return_sequences=True)(emb)\n",
    "    x_avg = GlobalMaxPooling1D()(x)\n",
    "    y = CuDNNLSTM(len(label_index)//2, return_sequences=True)(emb)\n",
    "    y_avg = GlobalMaxPooling1D()(y)\n",
    "    p = Concatenate(axis=1)([x_avg,y_avg]) \n",
    "    x = Attention(MAX_SEQUENCE_LENGTH)(x)\n",
    "    y = Attention(MAX_SEQUENCE_LENGTH)(y)\n",
    "    q = Concatenate(axis=1)([x,y])\n",
    "    z = Concatenate(axis=-1)([p,q])\n",
    "    z = Dropout(0.15)(z)\n",
    "    z = BatchNormalization()(z)\n",
    "    z = Dense(len(label_index), activation=\"relu\")(z)\n",
    "    z = Dropout(0.15)(z)\n",
    "    z = BatchNormalization()(z)\n",
    "    z = Dense(len(label_index), activation=\"sigmoid\")(z)\n",
    "    model = Model(inputs=inp, outputs=z)\n",
    "    return model\n",
    "\n",
    "#https://github.com/arthurdouillard/keras-snapshot_ensembles/blob/master/example.py\n",
    "def load_ensemble(folder, keep_last=None):\n",
    "    paths = glob.glob(os.path.join(folder, 'weights_cycle_*.h5'))\n",
    "    print('Found:', ', '.join(paths))\n",
    "    if keep_last is not None:\n",
    "        paths = sorted(paths)[-keep_last:]\n",
    "    print('Loading:', ', '.join(paths))\n",
    "\n",
    "    x_in = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "    outputs = []\n",
    "\n",
    "    for i, path in enumerate(paths):\n",
    "        m = get_model(input_layer=x_in)\n",
    "        m.load_weights(path)\n",
    "        outputs.append(m.output)\n",
    "\n",
    "    shape = outputs[0].get_shape().as_list()\n",
    "    x = Lambda(lambda x: K.mean(K.stack(x, axis=0), axis=0),\n",
    "               output_shape=lambda _: shape)(outputs)\n",
    "    model = Model(inputs=x_in, outputs=x)\n",
    "    return model\n",
    "\n",
    "EMBEDDING_FILE = EMB_BASE + 'matrix_glove_punc_brute'\n",
    "with open(EMBEDDING_FILE, 'rb') as f:\n",
    "    embedding_matrix = pickle.load(f)\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    y_pred = K.round(y_pred)\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'))\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'))\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'))\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'))\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "def dice_coef(y_true, y_pred, smooth=0):\n",
    "    intersection = K.sum(y_true * y_pred)\n",
    "    union = K.sum(y_true) + K.sum(y_pred)\n",
    "    return K.mean( (2. * intersection + smooth) / (union + smooth))\n",
    "def dice_p_bce(in_gt, in_pred):\n",
    "    return binary_crossentropy(in_gt, in_pred) - 0.1*dice_coef(in_gt, in_pred,smooth=1)\n",
    "\n",
    "model = get_model(emb_w=embedding_matrix)\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss=dice_p_bce, optimizer=RMSprop(lr=1e-3), metrics=[f1])\n",
    "snapshot = Snapshot('snapshots1', nb_epochs=12, verbose=1, nb_cycles=3)\n",
    "\n",
    "model.fit_generator(\n",
    "    train_gen,\n",
    "    steps_per_epoch=len(train_df)//BATCH_SIZE,\n",
    "    epochs=4,\n",
    "    verbose=1,\n",
    "    validation_data=(valid_x,valid_y),)\n",
    "\n",
    "model.fit_generator(\n",
    "    train_gen,\n",
    "    steps_per_epoch=len(train_df)//BATCH_SIZE,\n",
    "    epochs=12,\n",
    "    verbose=1,\n",
    "    callbacks=[snapshot],\n",
    "    validation_data=(valid_x,valid_y),)\n",
    "\n",
    "model = load_ensemble('snapshots1')\n",
    "valid_preds = model.predict(valid_x,verbose=1)\n",
    "thresholds = np.linspace(0,1,200)\n",
    "max_f1 = -1\n",
    "best_thres = -1\n",
    "for thres in tqdm(thresholds):\n",
    "    f1 = f1_score(valid_y,valid_preds>thres,average='micro')\n",
    "    if f1 > max_f1:\n",
    "        max_f1 = f1\n",
    "        best_thres = thres\n",
    "print(max_f1,best_thres)\n",
    "pred_test_y = model.predict([test_x], batch_size=256, verbose=1)\n",
    "\n",
    "pred_test_y = (pred_test_y>best_thres).astype(int)\n",
    "label_index_inverse = {val:key for key,val in label_index.items()}\n",
    "def vector_to_tags(vec):\n",
    "    tags = []\n",
    "    for i,v in enumerate(vec):\n",
    "        if v == 1:\n",
    "            tags.append(label_index_inverse[i])\n",
    "    return '|'.join(tags)\n",
    "vector_to_tags(pred_test_y[2]>best_thres)\n",
    "\n",
    "df_test['tags'] = [vector_to_tags(pred>best_thres) for pred in tqdm(pred_test_y)]\n",
    "df_test.drop(columns=['texts'],inplace=True)\n",
    "df_test.to_csv('result.csv',index=False)\n",
    "df_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
