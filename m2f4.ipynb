{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras_contrib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-f63e6aa33d8a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mFastText\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcopy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mkeras_contrib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras_contrib'"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pickle\n",
    "import numpy as np\n",
    "import itertools\n",
    "import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import FastText\n",
    "from copy import deepcopy\n",
    "import keras_contrib\n",
    "from sklearn.metrics import f1_score,accuracy_score\n",
    "from keras.models import load_model\n",
    "from gensim.models import Word2Vec\n",
    "from more_itertools import locate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    #Read preprocessed tags\n",
    "    with open(\"tags.pkl\", \"rb\") as fp:\n",
    "        tags = pickle.load(fp)\n",
    "    \n",
    "    #Read preprocessed train_paras.txt\n",
    "    with open('train_paras.txt','r') as f:\n",
    "        data=f.readlines()\n",
    "\n",
    "    for i in range(0,len(data)):\n",
    "        data[i]=data[i][2:].split(\"', '\")\n",
    "        data[i][-1]=data[i][-1].replace(\"']\\n\",'')\n",
    "    \n",
    "    #Read one hot target\n",
    "    with open('one_hot_target.txt','r') as f:\n",
    "        full_target=f.readlines()\n",
    "\n",
    "    target=[]\n",
    "    i=0\n",
    "    k=0\n",
    "    while(i<len(full_target)):\n",
    "        new_sen=[]\n",
    "        for j in range(0,len(data[k])):\n",
    "            x=full_target[i+j].replace(\"[\",'')\n",
    "            x=x.replace(\"]\\n\",'')\n",
    "            x=x.split(' ')\n",
    "            x=[float(num) for num in x if num not in \"\"]\n",
    "            new_sen.append(x)\n",
    "        i+=len(data[k])\n",
    "        k+=1\n",
    "        target.append(np.array(new_sen))\n",
    "    \n",
    "    #Convert target to list\n",
    "    for i in range(0,len(target)):\n",
    "        target[i]=target[i].tolist()\n",
    "    \n",
    "    #Load fasttext and word2vec models\n",
    "    fasttext_model = FastText.load('fasttext.model')\n",
    "    w2v_model=Word2Vec.load('word2vec.model') \n",
    "    \n",
    "    #Padding both data and target to fixed size of 300\n",
    "    data,target=padding(data,target)\n",
    "    \n",
    "    #Calling train validation split\n",
    "    x_train,y_train,x_test,y_test,tags_test=train_test_split(data,target)\n",
    "    \n",
    "    #Building model architecture\n",
    "    model=model_define()\n",
    "    \n",
    "    #Fitting the model for training data\n",
    "    model.fit_generator(generator=generator(64,x_train,y_train),epochs=1,steps_per_epoch=len(data)/64)\n",
    "    \n",
    "    #Saving model weights\n",
    "    model.save_weights('keras_full_model_weights.h5')\n",
    "    \n",
    "    #Sorting tags based on frequency\n",
    "    set_of_all_tags=sorted_tags(tags)\n",
    "    \n",
    "    #Predicting tags on test data\n",
    "    predicted_tags=final_pred(x_test,model,set_of_all_tags,w2v_model)\n",
    "    \n",
    "    #Combining predicted tags with tags predicted from code_classification\n",
    "    predicted_tags=code_classify(x_test,predicted_tags)\n",
    "    \n",
    "    #Computing f1 score\n",
    "    avg_f1=0\n",
    "    for i in range(0,len(x_test)):\n",
    "        avg_f1+=f1score(predicted_tags[i],tags_test[i])\n",
    "    print(avg_f1/len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(data,target):\n",
    "    \n",
    "    #iterating over each sentence\n",
    "    for i in range(0,len(data)):\n",
    "        \n",
    "        #slicing to 300 size para\n",
    "        data[i]=data[i][0:300]\n",
    "        leng=300-len(data[i])\n",
    "        #padding\n",
    "        data[i]=data[i]+leng*['pad']\n",
    "        #slicing target to 300 size\n",
    "        target[i]=target[i][0:300]\n",
    "        target[i]+=([[1.,  0.,  0.,  0.,  0.]]*leng)\n",
    "    \n",
    "    #returning 300 sized paragraph and target\n",
    "    return data,target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(data,target):\n",
    "    \n",
    "    #80% of data for training\n",
    "    x_train=data[0:int(0.80*len(data))]\n",
    "    y_train=target[0:int(0.80*len(data))]\n",
    "    \n",
    "    #20% of data for validation\n",
    "    x_test=data[len(x_train):]\n",
    "    y_test=target[len(x_train):]\n",
    "\n",
    "    tags_test=tags[len(x_train):]\n",
    "    \n",
    "    return x_train,y_train,x_test,y_test,tags_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_define():\n",
    "    \n",
    "    #Architecture of the model\n",
    "    input_text = keras.layers.Input(shape=(300,300))\n",
    "    lstm_1 = keras.layers.Bidirectional(keras.layers.LSTM(units=128, return_sequences=True))(input_text)\n",
    "    drop = keras.layers.Dropout(0.2)(lstm_1)\n",
    "    dense = keras.layers.TimeDistributed(keras.layers.Dense(50, activation=\"relu\"))(drop)\n",
    "    out=keras.layers.TimeDistributed(keras.layers.Dense(5,activation='softmax'))(dense)\n",
    "    \n",
    "    #Defining the model\n",
    "    model = keras.Model(inputs = input_text,outputs=out)\n",
    "    \n",
    "    #Compiling the model\n",
    "    model.compile(optimizer=\"adam\", loss='categorical_crossentropy')\n",
    "    \n",
    "    return model\n",
    "    #model.load_weights('keras_full_model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(batch_size,from_list_x,from_list_y):\n",
    "    \n",
    "    #Generator function model training\n",
    "    assert len(from_list_x) == len(from_list_y)\n",
    "    total_size = len(from_list_x)\n",
    "\n",
    "    while True:\n",
    "\n",
    "        for i in range(0,total_size,batch_size):\n",
    "            yield np.array(transformation(deepcopy(from_list_x[i:i+batch_size]))),np.array(from_list_y[i:i+batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformation(x):\n",
    "    \n",
    "    #Extracting word2vec and fasttext vectors for each word in para\n",
    "    for i in range(len(x)):\n",
    "        for j in range(300):\n",
    "            try:\n",
    "                #word2vec feature\n",
    "                w2v = w2v_model.wv[x[i][j]]\n",
    "            except:\n",
    "                w2v = np.zeros([150])\n",
    "            try:\n",
    "                #fasttext feature\n",
    "                ft = fasttext_model.wv[x[i][j]]\n",
    "            except:\n",
    "                ft = np.zeros([150])\n",
    "            \n",
    "            #Concatenating word2vec and fasttext features\n",
    "            x[i][j] = np.concatenate([w2v,ft])\n",
    "            \n",
    "    return np.array(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorted_tags(tags):\n",
    "    merged = list(itertools.chain.from_iterable(tags))\n",
    "\n",
    "    dictionary_tags={}\n",
    "    for i in range(0,len(merged)):\n",
    "        try:\n",
    "            dictionary_tags[merged[i]]+=1\n",
    "        except:\n",
    "            dictionary_tags[merged[i]]=1\n",
    "        \n",
    "    sorted_by_value = sorted(dictionary_tags.items(), key=lambda kv: kv[1],reverse=True)\n",
    "\n",
    "    set_of_all_tags=[]\n",
    "    for i in range(0,len(sorted_by_value)):\n",
    "        if sorted_by_value[i][0] not in set_of_all_tags:\n",
    "            set_of_all_tags.append(sorted_by_value[i][0])\n",
    "            \n",
    "    return set_of_all_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_lookup(word,set_of_all_tags,w2v_model):\n",
    "    try:\n",
    "        values = [i[0] for i in w2v_model.wv.most_similar(word)]\n",
    "    except KeyError:\n",
    "        return 0\n",
    "    \n",
    "    for i in range(len(values)):\n",
    "        try:\n",
    "            lookup = set_of_all_tags.index(values[i])\n",
    "            return set_of_all_tags[lookup]\n",
    "        except ValueError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def look_up_two(word,set_of_all_tags):\n",
    "    for i in range(0,len(set_of_all_tags)):\n",
    "        x=set_of_all_tags[i].split('-')\n",
    "        if word in x and len(x)==2:\n",
    "            return [set_of_all_tags[i]]\n",
    "    return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def look_up_three(word,set_of_all_tags):\n",
    "    for i in range(0,len(set_of_all_tags)):\n",
    "        x=set_of_all_tags[i].split('-')\n",
    "        if word in x and len(x)==3:\n",
    "            return [set_of_all_tags[i]]\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(pred_data,model,set_of_all_tags,w2v_model):\n",
    "    \n",
    "    dat = transformation(deepcopy(pred_data))\n",
    "    result = model.predict(dat)\n",
    "    \n",
    "    new_res=[]\n",
    "    for i in range(0,len(result)):\n",
    "        pred_data[i]=np.array(pred_data[i])\n",
    "        res=[]\n",
    "        for j in range(0,len(result[i])):\n",
    "            res.append(np.argmax(result[i][j]))\n",
    "        new_res.append(res) \n",
    "        \n",
    "    del result\n",
    "    \n",
    "    main_tags = []\n",
    "    sub_tags = []\n",
    "    \n",
    "    for i in range(0,len(new_res)):\n",
    "        main_tags.append(pred_data[i][np.where(np.array(new_res[i])==1)[0]])\n",
    "        sub_tags.append(pred_data[i][np.where(np.array(new_res[i])==2)[0]])\n",
    "        \n",
    "    sub_tags_all=[]\n",
    "    for i in range(0,len(sub_tags)):\n",
    "        sub_tags_temp = []\n",
    "        j=0\n",
    "        while(j<len(sub_tags[i])):\n",
    "            z = reverse_lookup(sub_tags[i][j],set_of_all_tags,w2v_model)\n",
    "            if z != 0:\n",
    "                sub_tags_temp.append(z)\n",
    "            j+=1\n",
    "        sub_tags_all.append(sub_tags_temp)\n",
    "    \n",
    "    \n",
    "    two_tags_all=[]\n",
    "    for i in range(0,len(new_res)):\n",
    "        two_tags_indices=np.where(np.array(new_res[i])==3)[0]\n",
    "        two_tags_temp=[]\n",
    "        for j in range(0,len(two_tags_indices)-1):\n",
    "            doub=look_up_two(pred_data[i][two_tags_indices[j]],set_of_all_tags)\n",
    "            try:\n",
    "                dou=doub[0].split('-')\n",
    "                if dou[1]==pred_data[i][two_tags_indices[j+1]]:\n",
    "                    two_tags_temp+=doub\n",
    "            except:\n",
    "                pass\n",
    "        two_tags_all.append(two_tags_temp)\n",
    "            \n",
    "        \n",
    "    three_tags_all=[]\n",
    "    for i in range(0,len(new_res)):\n",
    "        three_tags_indices=np.where(np.array(new_res[i])==4)[0]\n",
    "        three_tags_temp=[]\n",
    "        for j in range(0,len(three_tags_indices)-1):\n",
    "            thre=look_up_three(pred_data[i][three_tags_indices[j]],set_of_all_tags)\n",
    "            try:\n",
    "                three=thre[0].split('-')\n",
    "                if three[1]==pred_data[i][three_tags_indices[j+1]] or three[2]==pred_data[i][three_tags_indices[j+2]]:\n",
    "                    three_tags_temp+=thre\n",
    "            except:\n",
    "                pass\n",
    "        three_tags_all.append(three_tags_temp)\n",
    "    \n",
    "    full_tags=[]\n",
    "    for i in range(0,len(pred_data)):        \n",
    "        full = list(main_tags[i]) + sub_tags_all[i]+two_tags_all[i]+three_tags_all[i]\n",
    "        full_tags.append(list(set(full)))\n",
    "    \n",
    "    \n",
    "    return full_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_pred(data,model,set_of_all_tags,w2v_model):\n",
    "    i=0\n",
    "    predicted_tags=[]\n",
    "    while(i<len(data)):\n",
    "        pred=predict(data[i:i+64],model,set_of_all_tags,w2v_model)\n",
    "        for j in range(0,len(pred)):\n",
    "            if 'angular' in pred[j]:\n",
    "                ind=pred[j].index('angular')\n",
    "                pred[ind]=['angularjs']\n",
    "            if 'ruby' in pred[j]:\n",
    "                ind=pred[j].index('ruby')\n",
    "                pred[ind]=['ruby-on-rails']\n",
    "        i+=64\n",
    "        predicted_tags+=pred\n",
    "    return predicted_tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def code_classify_model(data,predicted_tags):\n",
    "    for i in range(0,len(data)):\n",
    "        data[i]=\" \".join(word for word in data[i])\n",
    "\n",
    "    with open(\"vectorizer_1.pkl\", \"rb\") as fp:\n",
    "        vectorizer_1 = pickle.load(fp)\n",
    "    with open(\"vectorizer_2.pkl\", \"rb\") as fp:\n",
    "        vectorizer_2 = pickle.load(fp)\n",
    "    with open(\"model_codes_1.pkl\", \"rb\") as fp:\n",
    "        model_1 = pickle.load(fp)\n",
    "    with open(\"model_codes_2.pkl\", \"rb\") as fp:\n",
    "        model_2 = pickle.load(fp)\n",
    "\n",
    "    for i in range(0,len(data)):\n",
    "        prob1=model_1.predict_proba(vectorizer_1.transform([data[i]]))\n",
    "        prob2=model_2.predict_proba(vectorizer_2.transform([data[i]]))\n",
    "        ind1=np.argmax(prob1)\n",
    "        ind2=np.argmax(prob2)\n",
    "        if prob1[0][ind1]>0.40:\n",
    "            predicted_tags[i].append((model_1.predict(vectorizer_1.transform([data[i]]))[0]))\n",
    "        elif prob2[0][ind2]>0.60:\n",
    "            predicted_tags[i].append((model_2.predict(vectorizer_2.transform([data[i]]))[0]))\n",
    "    return predicted_tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
